{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plan list\n",
    "- Other parameters: Sextapole, rf power. Finally oven parameters. \n",
    "\n",
    "    Should be able to just add them. Need the scaling function for new variables. \n",
    "\n",
    "- Guide the optimization.\n",
    "\n",
    "    ```\n",
    "    optimizer.probe(params)\n",
    "    ```\n",
    "\n",
    "- Avoid **danger zone** by monitoring the heater power.\n",
    "\n",
    "    > x-ray -> temp of heliem -> amount of gas -> pressure -> ice -> liquid heliem heater power (final diagnosis of x-ray). We really want to avoid x-rays\n",
    "\n",
    "    Add heater power to `venus.monitor()` and implement `venus.get_heater_power()`\n",
    "    ```python\n",
    "    def black_box_function(A, B, C):\n",
    "        venus.set_mag_currents(A, B, C)\n",
    "        ...\n",
    "        while time.time() < t_end:\n",
    "            venus.monitor(t_start,t_program_start,writefile,readvars,writefilefull)\n",
    "            if venus.get_heater_power() > threshold:\n",
    "                #TODO: reset the parameters\n",
    "                return - xray_cost\n",
    "        ...\n",
    "\n",
    "        #Monitor while measuring the beam current\n",
    "        return v_mean - instability_cost\n",
    "    ```\n",
    "\n",
    "- Consider \"momentum\". Early ending. \n",
    "\n",
    "    > Bias disk is fastest. Solonoid has induction. Gas pressure. \n",
    "\n",
    "    Modify the accquisition function of Bayes Optimization.\n",
    "\n",
    "    ```python\n",
    "    def _ucb(x, gp, kappa):\n",
    "        ...\n",
    "        return mean + kappa * std\n",
    "    ```\n",
    "\n",
    "    ```python\n",
    "    def _ucb(x, gp, kappa):\n",
    "        ...\n",
    "        return mean + kappa * std - np.dot(epsilon, displacement)\n",
    "        # epsilon and displacement are dim-d vectors in the parameter space\n",
    "    ```\n",
    "\n",
    "    simulate:\n",
    "    assume t1 = d * c1\n",
    "    inj 30s/uA\n",
    "    ext 20s/uA\n",
    "    mid 10s/uA\n",
    "\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2023.1.6 \n",
    "\n",
    "### Taking varying cost into account in Bayes Optimization\n",
    "- Expected Improvement (EI) is more commonly used than Upper Confidence Bound (UCB) as an acquisition function and has some theortical basis for convergence [Vazquez et al., 2010](https://www.sciencedirect.com/science/article/pii/S0378375810001850). \n",
    "    $$EI(x)=\\mathbb{E} [max(f(x)−f(x_+),0) ]$$\n",
    "    where $f$ is the model, $x_+$ is the current best parameter. \n",
    "    \n",
    "    Sometimes a term is added to facilitate exploration:\n",
    "    $$EI(x)=\\mathbb{E} [max(f(x)−f(x_+)-\\xi,0) ]$$\n",
    "    [Krasser, 2018](http://krasserm.github.io/2018/03/21/bayesian-optimization/#:~:text=A%20recommended%20default,.) recommended $\\xi$ to be 0.01 but this seems arbitrary to me and other papers have been using $\\xi=0$ so we could ignore this term for now. \n",
    "- Expected Improvement Per Unit cost [Snoek et al., 2012](https://papers.nips.cc/paper/2012/file/05311655a15b75fab86956663e1819cd-Paper.pdf)\n",
    "    $$EIpu(x) = \\frac{EI(x)}{c(x)}$$\n",
    "    \n",
    "- The problem with EIPU in the context of ML is that model training cost $c$ is fixed so it doesn't necessarilly converge to the best parameters. [Lee et al., 2020](https://assets.amazon.science/53/43/5121e84a45c7965cfd0644d81ec1/cost-aware-bayesian-optimization.pdf) proposed adding a **cost-cooled optimization phase** \n",
    "    \n",
    "    However, in our case, $EIpu(x_{i+1})$ is going to change based on where we probed last. A better way to write it is $EIpu(x_{i+1};x_i)$\n",
    "    \n",
    "    It is also meaningful (the expected amount of improvement per unit time spent) and is a great indicator for terminating the search. \n",
    "\n",
    "### Initial Probing\n",
    "- Draw 4 random points $\\mathcal{N}(0, I_d)$ around the best parameters found last time. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.13 (main, Aug 25 2022, 23:26:10) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e56f2290798654aa94e5f0ce144dd810b39b32133dcb0274ee15ee53844229cd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
